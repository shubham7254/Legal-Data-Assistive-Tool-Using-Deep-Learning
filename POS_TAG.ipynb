{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b71b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pranav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdf9bc275fa48ee8cf5f7f5d92028fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM \n",
    "from rouge import Rouge\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "rouge = Rouge()\n",
    "# Pre-process the input text to extract part of speech information\n",
    "def get_pos_tags(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    return pos_tags\n",
    "# Add the part of speech information as additional input to the model\n",
    "def add_pos_tags_to_input(input_text, pos_tags):\n",
    "    pos_tag_map = {\"NN\": \"<NN>\", \"JJ\": \"<JJ>\", \"VB\": \"<VB>\"}\n",
    "    pos_tagged_text = \"\"\n",
    "    for word, pos in zip(nltk.word_tokenize(input_text), pos_tags):\n",
    "        if pos in pos_tag_map:\n",
    "            pos_tagged_text += word + \" \" + pos_tag_map[pos] + \" \"\n",
    "        else:\n",
    "            pos_tagged_text += word + \" \"\n",
    "    return pos_tagged_text\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# Load the summaries.csv file into a dataframe\n",
    "df = pd.read_csv('new_summaries.csv')\n",
    "# Loop over the rows in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Text']\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    pos_tagged_text = add_pos_tags_to_input(text, pos_tags)\n",
    "    input_tokenized = tokenizer.encode(pos_tagged_text, return_tensors='pt',max_length=1024,truncation=True)\n",
    "    summary_ids = model.generate(input_tokenized,\n",
    "                                num_beams=9,\n",
    "                                no_repeat_ngram_size=3,\n",
    "                                length_penalty=2.0,\n",
    "                                min_length=150,\n",
    "                                max_length=500,\n",
    "                                early_stopping=True)\n",
    "    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
    "    # Update the 'Predicted Summary' column in the dataframe with the generated summary\n",
    "    df.at[index, 'Predicted Summary'] = summary\n",
    "    original_summary = row['Original Summary']\n",
    "    # Calculate the Rouge score\n",
    "    scores = rouge.get_scores(summary, original_summary)\n",
    "    # Add the Rouge R1, R2, and RL scores to the respective columns in the csv\n",
    "    df.at[index, 'R1'] = scores[0]['rouge-1']['f']\n",
    "    df.at[index, 'R2'] = scores[0]['rouge-2']['f']\n",
    "    df.at[index, 'Rl'] = scores[0]['rouge-l']['f']    \n",
    "df.to_csv('Output_Pegasus_With_POS_TAG_100doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e76ec6",
   "metadata": {},
   "source": [
    "# Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the average scores\n",
    "r1_avg = df['R1'].mean()\n",
    "r2_avg = df['R2'].mean()\n",
    "rl_avg = df['Rl'].mean()\n",
    "\n",
    "# Print the average scores\n",
    "print(f\"Average R1 Score: {r1_avg}\")\n",
    "print(f\"Average R2 Score: {r2_avg}\")\n",
    "print(f\"Average Rl Score: {rl_avg}\")\n",
    "df.to_csv(\"Output_Pegasus_With_POS_TAG_100doc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d62d3",
   "metadata": {},
   "source": [
    "# Precision, Recall and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "\n",
    "df = pd.read_csv('Output_Pegasus_With_POS_TAG_100doc.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    predicted_summary = row['Predicted Summary']\n",
    "    original_summary = row['Original Summary']\n",
    "    \n",
    "    predicted_tokens = set(word_tokenize(predicted_summary))\n",
    "    original_tokens = set(word_tokenize(original_summary))\n",
    "\n",
    "    precision_score = precision(original_tokens, predicted_tokens)\n",
    "    recall_score = recall(original_tokens, predicted_tokens)\n",
    "    f1_score = f_measure(original_tokens, predicted_tokens)\n",
    "    \n",
    "    df.at[index, 'Precision'] = precision_score\n",
    "    df.at[index, 'Recall'] = recall_score\n",
    "    df.at[index, 'F1 Score'] = f1_score\n",
    "\n",
    "df.to_csv('Output_Pegasus_With_POS_TAG_100doc.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Output_Pegasus_With_POS_TAG_100doc.csv')\n",
    "\n",
    "precision_mean = df['Precision'].mean()\n",
    "recall_mean = df['Recall'].mean()\n",
    "f1_score_mean = df['F1 Score'].mean()\n",
    "\n",
    "print(\"Precision mean:\", precision_mean)\n",
    "print(\"Recall mean:\", recall_mean)\n",
    "print(\"F1 Score mean:\", f1_score_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f30f5",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e281484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "df = pd.read_csv('Output_Pegasus_With_POS_TAG_100doc.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    predicted_summary = row['Predicted Summary']\n",
    "    original_summary = row['Original Summary']\n",
    "    \n",
    "    predicted_tokens = word_tokenize(predicted_summary)\n",
    "    original_tokens = word_tokenize(original_summary)\n",
    "\n",
    "    bleu_score = sentence_bleu([original_tokens], predicted_tokens)\n",
    "    \n",
    "    df.at[index, 'BLEU Score'] = bleu_score\n",
    "\n",
    "df.to_csv('Output_Pegasus_With_POS_TAG_100doc.csv', index=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Output_Pegasus_With_POS_TAG_100doc.csv')\n",
    "\n",
    "bleu_score_mean = df['BLEU Score'].mean()\n",
    "\n",
    "print(\"BLEU Score mean:\", bleu_score_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a968cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
